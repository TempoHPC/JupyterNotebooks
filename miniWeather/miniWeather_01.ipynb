{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TempoHPC/JupyterNotebooks/blob/main/miniWeather/miniWeather_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TempoHPC**\n",
        "# \"Uso do padrão de paralelismo de linguagem em esquema de radiação da atmosfera\"\n",
        "### Instalação do miniWeather no Ubuntu 20.04 LTS"
      ],
      "metadata": {
        "id": "ND2kYHItEyQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Donwload and install NVIDIA HPC SDK"
      ],
      "metadata": {
        "id": "n8mICxHdmRKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Downloading and installing deb packages. This will take 5 minutes.\n",
        "! curl https://developer.download.nvidia.com/hpc-sdk/ubuntu/DEB-GPG-KEY-NVIDIA-HPC-SDK | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg\n",
        "! echo 'deb [signed-by=/usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' | sudo tee /etc/apt/sources.list.d/nvhpc.list\n",
        "! sudo apt-get update -y\n",
        "! sudo apt-get install -y nvhpc-22-11"
      ],
      "metadata": {
        "id": "iv_6sBm1vZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install `environment-modules` package to load `nvhpc`"
      ],
      "metadata": {
        "id": "Qu9UwS4Pme8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt install environment-modules"
      ],
      "metadata": {
        "id": "F0CRfGnjvgyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading `nvhpc` and run `nvaccelinfo` command"
      ],
      "metadata": {
        "id": "ffug2Yfkmuug"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcphTrlvvJkH"
      },
      "source": [
        "---\n",
        "Let's execute the cell below to display information about the GPUs running on the server by running the `nvaccelinfo` command, which ships with the NVIDIA HPC compiler that we will be using. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source /usr/share/modules/init/bash\n",
        "module use /opt/nvidia/hpc_sdk/modulefiles\n",
        "module load nvhpc/22.11\n",
        "nvaccelinfo"
      ],
      "metadata": {
        "id": "1Z_6ReluxYdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dDXsd4rvJkJ"
      },
      "source": [
        "The output of the above command will vary according to which GPUs you have in your system. For example, if you are running the lab on a machine with NVIDIA Tesla V100 GPUs, you might would see the following:\n",
        "\n",
        "```\n",
        "CUDA Driver Version:           10010\n",
        "NVRM version:                  NVIDIA UNIX x86_64 Kernel Module  418.87.01  Wed Sep 25 06:00:38 UTC 2019\n",
        "\n",
        "Device Number:                 0\n",
        "Device Name:                   Tesla V100-SXM2-16GB\n",
        "Device Revision Number:        7.0\n",
        "Global Memory Size:            16914055168\n",
        "Number of Multiprocessors:     80\n",
        "Concurrent Copy and Execution: Yes\n",
        "Total Constant Memory:         65536\n",
        "Total Shared Memory per Block: 49152\n",
        "Registers per Block:           65536\n",
        "Warp Size:                     32\n",
        "Maximum Threads per Block:     1024\n",
        "Maximum Block Dimensions:      1024, 1024, 64\n",
        "Maximum Grid Dimensions:       2147483647 x 65535 x 65535\n",
        "Maximum Memory Pitch:          2147483647B\n",
        "Texture Alignment:             512B\n",
        "Clock Rate:                    1530 MHz\n",
        "Execution Timeout:             No\n",
        "Integrated Device:             No\n",
        "Can Map Host Memory:           Yes\n",
        "Compute Mode:                  default\n",
        "Concurrent Kernels:            Yes\n",
        "ECC Enabled:                   Yes\n",
        "Memory Clock Rate:             877 MHz\n",
        "Memory Bus Width:              4096 bits\n",
        "L2 Cache Size:                 6291456 bytes\n",
        "Max Threads Per SMP:           2048\n",
        "Async Engines:                 6\n",
        "Unified Addressing:            Yes\n",
        "Managed Memory:                Yes\n",
        "Concurrent Managed Memory:     Yes\n",
        "Preemption Supported:          Yes\n",
        "Cooperative Launch:            Yes\n",
        "  Multi-Device:                Yes\n",
        "Default Target:            -ta=tesla:cc70\n",
        "```\n",
        "\n",
        "This gives us lots of details about the GPU, for instance the device number, the type of device, and at the very bottom the command line argument we should use when targeting this GPU (see *_NVIDIA HPC Compiler Option_*). We will use this command line option a bit later to build for our GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Creating user fortransp\n",
        "\n"
      ],
      "metadata": {
        "id": "ZauSBltE7BRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "adduser fortransp"
      ],
      "metadata": {
        "id": "OpsXjBevpynC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "su fortransp\n",
        "source /usr/share/modules/init/bash\n",
        "module use /opt/nvidia/hpc_sdk/modulefiles\n",
        "module load nvhpc/22.11"
      ],
      "metadata": {
        "id": "-i2uvTwR1MhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "su fortransp\n",
        "source /usr/share/modules/init/bash\n",
        "module use /opt/nvidia/hpc_sdk/modulefiles\n",
        "module load nvhpc/22.11\n",
        "\n",
        "cd\n",
        "cp -r -p /opt/nvidia/hpc_sdk/Linux_x86_64/22.11/examples/ ."
      ],
      "metadata": {
        "id": "pEZ9YAi9IH2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jacobi iteration example using Do Concurrent in Fortran"
      ],
      "metadata": {
        "id": "7LTdvID2DnKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build with to target NVIDIA GPU"
      ],
      "metadata": {
        "id": "ec1GDGQ1EEWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "su fortransp\n",
        "source /usr/share/modules/init/bash\n",
        "module use /opt/nvidia/hpc_sdk/modulefiles\n",
        "module load nvhpc/22.11\n",
        "\n",
        "cd\n",
        "cd examples/stdpar/jacobi\n",
        "nvfortran -stdpar -Minfo=accel -fast jacobi.f90 -o jacobi_gpu"
      ],
      "metadata": {
        "id": "xNdPij77CLis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "su fortransp\n",
        "source /usr/share/modules/init/bash\n",
        "module use /opt/nvidia/hpc_sdk/modulefiles\n",
        "module load nvhpc/22.11\n",
        "\n",
        "cd\n",
        "cd examples/stdpar/jacobi\n",
        "ls -ltr jacobi\n",
        "./jacobi_gpu"
      ],
      "metadata": {
        "id": "jhnK4wPYEvqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build with to target host multicore"
      ],
      "metadata": {
        "id": "zhvH4AN5EKp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "su fortransp\n",
        "source /usr/share/modules/init/bash\n",
        "module use /opt/nvidia/hpc_sdk/modulefiles\n",
        "module load nvhpc/22.11\n",
        "\n",
        "cd\n",
        "cd examples/stdpar/jacobi\n",
        "nvfortran -stdpar=multicore -Minfo=accel jacobi.f90 -o jacobi_multicore"
      ],
      "metadata": {
        "id": "w0IOBJ_rD6sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "su fortransp\n",
        "source /usr/share/modules/init/bash\n",
        "module use /opt/nvidia/hpc_sdk/modulefiles\n",
        "module load nvhpc/22.11\n",
        "\n",
        "cd\n",
        "cd examples/stdpar/jacobi\n",
        "ls -ltr jacobi\n",
        "./jacobi_multicore"
      ],
      "metadata": {
        "id": "QSK8RZAQDIbL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}